{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e565d097",
      "metadata": {
        "id": "e565d097"
      },
      "source": [
        "\n",
        "# Pipeline Deep Learning de génération d'images à partir de descriptions textuelles\n",
        "\n",
        "Ce notebook présente, étape par étape, le code correspondant au script oral de 7 minutes. Chaque cellule de texte (Markdown) explique la partie évoquée dans le script, suivie de la cellule de code Python associée.  \n",
        "La partie « Simulation avec modèle préentraîné » n’est **pas** incluse ici (vous avez déjà un programme pour cela).  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b45e943",
      "metadata": {
        "id": "4b45e943"
      },
      "source": [
        "\n",
        "##  Objectif du projet\n",
        "\n",
        "À partir d’un texte comme *\"un chat blanc portant des lunettes\"*, générer automatiquement une image correspondante.\n",
        "\n",
        "Ce pipeline couvre :  \n",
        "- La collecte des données  \n",
        "- Le prétraitement  \n",
        "- L'entraînement du modèle  \n",
        "- Le déploiement dans une interface web  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1276f22",
      "metadata": {
        "id": "a1276f22"
      },
      "source": [
        "\n",
        "##  Étape 1 : Choix du dataset\n",
        "\n",
        "Nous avons utilisé **Flickr8k**, contenant 8 000 images avec 5 descriptions chacune.  \n",
        "Avantages : léger, déjà aligné texte-image, bien documenté.\n",
        "\n",
        "> Pour respecter les contraintes de Google Colab, nous avons sélectionné un **sous-ensemble de 2 000 paires** texte-image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ed93502",
      "metadata": {
        "id": "2ed93502"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 1. Imports et Montage de Google Drive (pour sauvegarder les checkpoints)\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Montez votre Google Drive pour sauvegarder les checkpoints et éviter de perdre le travail\n",
        "drive.mount('/content/drive')\n",
        "save_dir = \"/content/drive/MyDrive/text_to_image_project\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Librairies principales\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2c77941",
      "metadata": {
        "id": "d2c77941"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Charger les légendes et chemins d'images depuis un CSV (Flickr8k)\n",
        "# (Assurez-vous d'avoir téléchargé les fichiers 'Flickr8k_text' sur Colab au préalable)\n",
        "df = pd.read_csv(\"/content/Flickr8k_text/Flickr8k.token.txt\",\n",
        "                 sep='\\t', names=['image', 'caption'])\n",
        "# Sélection aléatoire d'un sous-ensemble de 2 000 paires\n",
        "df = df.sample(n=2000, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Affichage rapide pour vérifier\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ea483f0",
      "metadata": {
        "id": "5ea483f0"
      },
      "source": [
        "\n",
        "##  Étape 2 : Prétraitement des données\n",
        "\n",
        "**Pour les images :**  \n",
        "- Redimension à 224×224 pixels  \n",
        "- Conversion en tenseur PyTorch  \n",
        "- Normalisation (valeurs entre 0 et 1)  \n",
        "\n",
        "**Pour les textes :**  \n",
        "- Tokenizer BERT (Hugging Face)  \n",
        "- Conserver `input_ids` et `attention_mask`  \n",
        "\n",
        "Nous créons une classe `FlickrDataset` qui, pour chaque exemple, renvoie :\n",
        "- `image_tensor`  \n",
        "- `input_ids`  \n",
        "- `attention_mask`  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0570c3c1",
      "metadata": {
        "id": "0570c3c1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Import du tokenizer et des transformations PyTorch\n",
        "from torchvision import transforms\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Définition du tokenizer BERT\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Transformation pour les images\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),             # Convertit en [0,1]\n",
        "])\n",
        "\n",
        "class FlickrDataset(Dataset):\n",
        "    def __init__(self, dataframe, image_dir, tokenizer, transform=None, max_length=32):\n",
        "        self.data = dataframe\n",
        "        self.image_dir = image_dir\n",
        "        self.tokenizer = tokenizer\n",
        "        self.transform = transform\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        img_filename = row['image'].split('#')[0]  # Flickr8k.token.txt a format \"IMG_####.jpg#0\"\n",
        "        img_path = os.path.join(image_dir, img_filename)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Tokenization du texte\n",
        "        encoding = self.tokenizer(\n",
        "            row['caption'],\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length\n",
        "        )\n",
        "        input_ids = encoding[\"input_ids\"].squeeze(0)        # (max_length,)\n",
        "        attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "        return image, input_ids, attention_mask\n",
        "\n",
        "# Chemin vers le dossier contenant les images Flickr8k (ex: '/content/Flickr8k_Dataset/Images')\n",
        "image_dir = \"/content/Flickr8k_Dataset/Images\"\n",
        "\n",
        "# Instanciation du dataset et du DataLoader\n",
        "dataset = FlickrDataset(df, image_dir, tokenizer, transform=image_transform)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a3f3a54",
      "metadata": {
        "id": "1a3f3a54"
      },
      "source": [
        "\n",
        "##  Étape 3 : Construction du modèle\n",
        "\n",
        "Nous définissons un modèle `DualBranchModel` qui contient :  \n",
        "- **Branche Image :** ResNet50 préentraîné (dernière couche remplacée par une projection linéaire vers 256 dimensions).  \n",
        "- **Branche Texte :** BERT préentraîné (on extrait `pooler_output`, puis projection en 256 dim).  \n",
        "\n",
        "L’objectif : les embeddings texte et image d’une même paire doivent être proches.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e50ef96f",
      "metadata": {
        "id": "e50ef96f"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "from torchvision.models import resnet50\n",
        "from transformers import BertModel\n",
        "\n",
        "class DualBranchModel(nn.Module):\n",
        "    def __init__(self, embed_dim=256):\n",
        "        super().__init__()\n",
        "        # Branche Image\n",
        "        self.image_encoder = resnet50(pretrained=True)\n",
        "        self.image_encoder.fc = nn.Linear(2048, embed_dim)\n",
        "\n",
        "        # Branche Texte\n",
        "        self.text_encoder = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        self.text_proj = nn.Linear(768, embed_dim)  # 768 = taille d'embedding BERT\n",
        "\n",
        "    def forward(self, images, input_ids, attention_mask):\n",
        "        # Extraction embedding image\n",
        "        image_embed = self.image_encoder(images)  # (batch_size, embed_dim)\n",
        "\n",
        "        # Extraction embedding texte\n",
        "        text_outputs = self.text_encoder(input_ids=input_ids,\n",
        "                                         attention_mask=attention_mask)\n",
        "        cls_embed = text_outputs.pooler_output       # (batch_size, 768)\n",
        "        text_embed = self.text_proj(cls_embed)       # (batch_size, embed_dim)\n",
        "\n",
        "        return image_embed, text_embed\n",
        "\n",
        "# Instancier le modèle\n",
        "model = DualBranchModel(embed_dim=256).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4833cdf",
      "metadata": {
        "id": "e4833cdf"
      },
      "source": [
        "\n",
        "##  Étape 4 : Entraînement du modèle — Limitations rencontrées\n",
        "\n",
        "On entraîne le modèle avec une **loss contrastive** (`CosineEmbeddingLoss`) pour rapprocher les embeddings correspondants.  \n",
        "On sauvegarde le modèle après chaque epoch pour reprendre en cas de déconnexion.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4587275",
      "metadata": {
        "id": "b4587275"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Définition de la fonction de perte\n",
        "loss_fn = nn.CosineEmbeddingLoss(margin=0.0)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "num_epochs = 3  # Réduire si nécessaire\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "    for images, input_ids, attention_mask in dataloader:\n",
        "        images = images.to(device)\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        image_embed, text_embed = model(images, input_ids, attention_mask)\n",
        "\n",
        "        # cibles = +1 pour chaque paire (on veut similarité élevée)\n",
        "        targets = torch.ones(images.size(0), device=device)\n",
        "        loss = loss_fn(image_embed, text_embed, targets)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} — Loss : {avg_loss:.4f}\")\n",
        "\n",
        "    # Sauvegarde du modèle après chaque epoch\n",
        "    checkpoint_path = os.path.join(save_dir, f\"dual_model_epoch{epoch+1}.pth\")\n",
        "    torch.save(model.state_dict(), checkpoint_path)\n",
        "    print(f\"→ Modèle sauvegardé dans : {checkpoint_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a87577d",
      "metadata": {
        "id": "5a87577d"
      },
      "source": [
        "\n",
        "##  Étape 6 : Déploiement dans une interface Web (Gradio)\n",
        "\n",
        "On recharge le modèle entraîné (dernier checkpoint) et on crée une fonction d’inférence qui,\n",
        "à partir d’une légende textuelle, renvoie l’image la plus proche dans l’espace.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c7348a7",
      "metadata": {
        "id": "6c7348a7"
      },
      "outputs": [],
      "source": [
        "\n",
        "import gradio as gr\n",
        "\n",
        "# Recharger le modèle entraîné (dernier checkpoint)\n",
        "model = DualBranchModel(embed_dim=256).to(device)\n",
        "last_checkpoint = os.path.join(save_dir, \"dual_model_epoch3.pth\")\n",
        "model.load_state_dict(torch.load(last_checkpoint))\n",
        "model.eval()\n",
        "\n",
        "# Pré-calcul des embeddings images pour le sous-ensemble\n",
        "image_embeddings = []\n",
        "image_paths = []\n",
        "\n",
        "for idx in range(len(dataset)):\n",
        "    img, input_ids, attention_mask = dataset[idx]\n",
        "    img = img.unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        emb, _ = model(img,\n",
        "                       input_ids.unsqueeze(0).to(device),\n",
        "                       attention_mask.unsqueeze(0).to(device))\n",
        "    image_embeddings.append(emb.cpu())\n",
        "    img_filename = df.iloc[idx]['image'].split('#')[0]\n",
        "    image_paths.append(os.path.join(image_dir, img_filename))\n",
        "\n",
        "image_embeddings = torch.cat(image_embeddings, dim=0)  # (2000, 256)\n",
        "\n",
        "def retrieve_image_from_caption(caption_text):\n",
        "    encoding = tokenizer(\n",
        "        caption_text, return_tensors=\"pt\",\n",
        "        padding=\"max_length\", truncation=True, max_length=32\n",
        "    )\n",
        "    input_ids = encoding[\"input_ids\"].to(device)\n",
        "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _, text_embed = model(None, input_ids, attention_mask)\n",
        "    text_embed = text_embed.cpu()  # (1, 256)\n",
        "\n",
        "    sims = torch.nn.functional.cosine_similarity(text_embed, image_embeddings)\n",
        "    best_idx = torch.argmax(sims).item()\n",
        "    best_image_path = image_paths[best_idx]\n",
        "\n",
        "    return Image.open(best_image_path).convert(\"RGB\")\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=retrieve_image_from_caption,\n",
        "    inputs=gr.inputs.Textbox(lines=2, placeholder=\"Entrez une description…\"),\n",
        "    outputs=\"image\",\n",
        "    title=\"Récupération d'image à partir d'une légende\"\n",
        ")\n",
        "\n",
        "iface.launch(share=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1cd0b32",
      "metadata": {
        "id": "b1cd0b32"
      },
      "source": [
        "\n",
        "## Conclusion\n",
        "\n",
        "1. **Collecte de données** : Flickr8k, sous-ensemble 2 000 paires  \n",
        "2. **Prétraitement** : redimension, normalisation, tokenisation BERT, extraction d’embeddings  \n",
        "3. **Modèle dual** : ResNet + BERT + projection linéaire  \n",
        "4. **Entraînement** : perte de similarité, sauvegarde des poids à chaque epoch  \n",
        "5. **Simulation** : dans un autre notebook avec Stable Diffusion  \n",
        "6. **Déploiement** : interface Gradio pour récupérer l’image la plus proche d’une légende  \n",
        "\n",
        "Ce notebook fonctionne “normalement” (sans la partie simulation) et permet de relier chaque étape à votre script de présentation.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b8c87a9",
      "metadata": {
        "id": "9b8c87a9"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}